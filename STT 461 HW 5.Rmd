---
title: "Statistical Inference"
author: "Derien Weatherspoon"
date: "2023-03-20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(MASS)
library(corrplot)
library(dplyr)
library(caret)
library(boot)
library(bayesboot)
library(ggcorrplot)
library(DirichletReg)
library(tree)
```


## Question 1
 A linear regression model prediction is in the form of y = 2.18x1 - 4.56x2. The root mean square error of the residuals is 0.856.
(a) What is the E(y|(x1,x2) = (2, -3)?
(b) What is the probability that y > 20, given that (x1,x2) = (2, -3)?

```{r Q1}
# setup values
x1 <- 2
x2 <- -3
y <- 2.18*x1 - 4.56*x2
rmse <- 0.856

#a 
Ey <- y # predicted value of y
Ey

#b 
z <- (20-y)/rmse
1-pnorm(z)
```

## Question 2

Take the Boston dataset, available in D2L. This data has information about different neighborhoods in Boston, and we will use it to predict the median housing price for the neighborhoods. Here is an explanation of the variables:

(a) Which other variable correlates the strongest with medv? Note that strongest mean largest absolute value, whether it’s positive or negative.
```{r Q2 a}
bos <- read.csv("Boston.csv")
bos_2 <- bos[, -c(4,9)] # remove categorical variables
round(cor(bos_2),
  digits = 2 # rounded to 2 decimals
)
ggcorrplot(cor(bos_2), hc.order = TRUE) # looks to lstat and medv due to the heatmap. 

plot(bos_2$medv, bos_2$lstat) # verifying the correlation between lstat and medv
round(cor(bos_2$medv, bos_2$lstat), digits = 3)
```
lstat is the variable correlated most with medv, with an absolute value score of 0.738. The next two variables in line are "rm" and "ptratio".

(b) Build a simple linear regression model with that variable as the x, with
i) Constructing the normal equations AtAx = Atb, and solving for the coefficient vector x.
ii) Using lm. Do the coefficients agree?
```{r Q2 b}
# i)
# build a simple linear regression model with lstat as x
A <- cbind(1,bos$lstat) # design matrix A
b <- bos$medv # response variable b
# solve for the coefficient vector x
x <- solve(t(A) %*% A) %*% t(A) %*% b
x
# ii)
# using lm now
bos.fit <- lm(medv ~ lstat, data = bos)
coefficients(bos.fit) #check coefficients
```
The coefficients match for both modeling methods, they are the same.

(c) Next, build a linear regression model with the 2 other variables that correlate most strongly with medv, using lm. How do the adjusted R-squared values compare between the models?
```{r Q2 c}
bos.fit_2 <- lm(medv ~ lstat + rm + ptratio, data = bos) # These two variables I mentioned earlier when checking correlation.
summary(bos.fit_2)
```
The adjusted R^2 gets higher in this model compared to the previous model, but this is only natural because the Adjusted R^2 only gets higher when you add more variables.

(d) For the model in (c), what are the 95% confidence intervals for the parameters, according to
the t-values?
```{r Q2 d}
confint(bos.fit_2, level = 0.95)
```

(e) For the model in (c), what are the 95% confidence intervals for the parameters, using (i)
regular bootstrapping, and (ii) Bayesian bootstrapping?
```{r Q2 e}
# Regular Bootstrap
coeff1 <- rep(0,100)
coeff2 <- rep(0,100)
weight <- rep(0,length(bos$rm))
for (i in 1:100){
  n <- length(bos$rm)
  row_sample <- sample(1:n, n, replace = T)
  bos_sample <- bos[row_sample,]
  mod <- lm(medv ~ rm + ptratio, data = bos_sample)
  coeff1[i] <- mod$coefficients[1]
  coeff2[i] <- mod$coefficients[2]
}
quantile(coeff1, c(0.025, 0.975))
quantile(coeff2, c(0.025, 0.975))

# Bayesian Bootstrap
for (i in 1:100){
  n <- length(bos$rm)
  weight <- rdirichlet(1, rep(1,n))
  row_sample <- sample(1:n, n, replace = T, prob = weight)
  bos_sample <- bos[row_sample,]
  mod <- lm(medv ~ rm + ptratio, data = bos_sample)
  coeff1[i] <- mod$coefficients[1]
  coeff2[i] <- mod$coefficients[2]
}
quantile(coeff1, c(0.025, 0.975))
quantile(coeff2, c(0.025, 0.975))
```

## Question 3
Take a look at the nndb dataset available in the sample data. There are 45 columns, 38 of which are numerical. We will build a PCA regression model for the Energy_kcal variable. After doing part (c), Transform the PCA regression equation into the original coordinates. In terms of sensitivity analysis which original variable is the most significant?


(a) Find the covariance matrix and the principal component values of the numerical fields, excluding the Energy_kcal field. How many of the 37 principal components are within 0.1% of the largest component?
```{r Q3 a}
nndb <- read.csv("nndb_flat.csv")

numericVars <- select_if(nndb, is.numeric)  
# Calculate covariance matrix
cov_mat <- cov(numericVars)
summary(princomp(cov_mat))
```

(b) Transform the data into the principal component basis. Confirm that the data in the new basis has the multicollinearity removed.
```{r Q3 b}
# Calculate principal components
pca <- prcomp(numericVars[-2], scale. = TRUE)

# Calculate proportion of variance explained by each principal component
var_prop <- pca$sdev^2 / sum(pca$sdev^2)

# Count the number of components within 0.1% of the largest proportion
num_components <- sum(var_prop >= 0.999 * max(var_prop))
```

(c) Perform a linear regression using only the principal components which are within 0.1% in size of the largest component.
```{r Q3 c}

```

## Question 4
For the churn dataset, we will model the churn (whether a customer left) based on different models. Consider the fields Age, Total_Purchase, Account_Manager, Years, and Num_Sites as possible X variables. Note that Account_Manager is a binary categorical variable.

(a) Create histograms to examine how each variable might predict churn.
```{r Q4 a}
churn <- read.csv("customer_churn.csv")
hist(churn$Age)
hist(churn$Total_Purchase)
hist(churn$Years)
hist(churn$Num_Sites)

hist(churn$Account_Manager)
```

(b) Split the data into train and test datasets.
```{r Q4 b}
rows <- 1:nrow(churn)
train_split <- sample(rows, 0.7*length(rows))
test_split <- rows[-train_split]
churn_train <- churn[train_split,]
churn_test <- churn[test_split,]
```

(c) Fit a logistic regression model—first with all X’s, and then remove those X’s that are not
statistically significant. Create a confusion matrix for this model.
```{r Q4 c}
churn.fit <- glm(Churn ~ Age + Years + Num_Sites , data = churn_train, family = binomial)
summary(churn.fit)

churn_pred <- predict(churn.fit, newdata = churn_test, type = "response")
churn_pred_class <- ifelse(churn_pred > 0.5, 1, 0)

# confusion matrix
confusionMatrix(table(churn_pred_class, churn_test$Churn))
```
The variable *Total_Purchase* was the variable I removed because it was not significant.

(d) Create a decision tree model. Create a confusion matrix for this model.
```{r Q4 d}
churn_tree <- tree(as.factor(Churn) ~ Age + Years + Num_Sites , data = churn_train)
summary(churn_tree)
plot(churn_tree)
text(churn_tree)
churn_tree_pred <- predict(churn_tree, churn_test, type = 'class')

# confusion matrix

confusionMatrix(table(churn_tree_pred, churn_test$Churn))
```

